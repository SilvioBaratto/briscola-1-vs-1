# Briscola RL

**Deep Reinforcement Learning for the Classic Italian Card Game**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.3+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)

<p align="center">
  <img src="briscola_game.gif" alt="Briscola RL Agent vs Random Opponent" width="600">
  <br>
  <em>RL Agent (bottom) playing against a random opponent</em>
</p>

---

## What is This Project?

This project trains an AI agent to play **Briscola**, Italy's most popular card game, using modern reinforcement learning techniques. The agent learns by playing thousands of games against an LLM-powered opponent, gradually discovering optimal strategies through trial and error.

**Key Features:**
- Custom PPO (Proximal Policy Optimization) implementation from scratch
- Trick history encoding for temporal reasoning (inspired by DouZero/RLCard)
- LLM opponent powered by Ollama for diverse gameplay
- Full-stack web application to play against the trained AI
- Comprehensive reward shaping based on Briscola strategy

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [How It Works](#how-it-works)
3. [The Game: Briscola](#the-game-briscola)
4. [Technical Deep Dive](#technical-deep-dive)
5. [Web Application](#web-application)
6. [Project Structure](#project-structure)
7. [Configuration](#configuration)
8. [Contributing](#contributing)

---

## Quick Start

### Prerequisites

| Requirement | Version | Purpose |
|-------------|---------|---------|
| Python | 3.10+ | Core runtime |
| Ollama | Latest | LLM opponent |
| Node.js | 18+ | Frontend (optional) |
| Docker | Latest | Containerized deployment (optional) |

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/briscola-1-vs-1.git
cd briscola-1-vs-1

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install the package
pip install -e .

# 4. Setup LLM opponent
ollama pull mistral:7b
ollama create briscola -f Modelfile

# 5. Generate BAML client
baml-cli generate
```

### Train Your First Agent

```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start training
briscola train
```

You'll see output like:
```
Training... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   15% WR: 72.0% | R: 0.89 | PL: -0.023
```

| Metric | Meaning |
|--------|---------|
| `WR` | Win Rate - percentage of games won |
| `R` | Reward - average cumulative reward per game |
| `PL` | Policy Loss - negative means the agent is improving |

---

## How It Works

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRAINING LOOP                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚           â”‚    â”‚           â”‚    â”‚           â”‚               â”‚
â”‚  â”‚  Briscola â”‚â—„â”€â”€â–ºâ”‚    PPO    â”‚â—„â”€â”€â–ºâ”‚    LLM    â”‚               â”‚
â”‚  â”‚   Game    â”‚    â”‚   Agent   â”‚    â”‚  Opponent â”‚               â”‚
â”‚  â”‚  Engine   â”‚    â”‚           â”‚    â”‚  (Ollama) â”‚               â”‚
â”‚  â”‚           â”‚    â”‚           â”‚    â”‚           â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚       â”‚                â”‚                                        â”‚
â”‚       â–¼                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚Observationâ”‚    â”‚  Actor-   â”‚                                â”‚
â”‚  â”‚  Vector   â”‚    â”‚  Critic   â”‚                                â”‚
â”‚  â”‚ (533 dim) â”‚    â”‚  Network  â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Training Process

1. **Game Setup**: Environment deals cards, reveals trump (briscola)
2. **Observation**: Agent sees its hand, trump suit, played cards, scores
3. **Action**: Agent selects one of 3 cards to play
4. **Opponent**: LLM analyzes the game state and responds
5. **Reward**: Agent receives feedback based on trick outcome
6. **Learning**: PPO updates the neural network weights
7. **Repeat**: Process continues for thousands of games

### Why PPO?

**Proximal Policy Optimization** is ideal for card games because:

| Challenge | How PPO Helps |
|-----------|---------------|
| High variance outcomes | Clipping prevents overreacting to lucky/unlucky games |
| Sparse rewards | GAE provides dense learning signals |
| Invalid actions | Action masking handles variable hand sizes |
| Sample efficiency | Multiple epochs over collected experience |

---

## The Game: Briscola

### Overview

Briscola is a trick-taking game for 2-4 players using a 40-card Italian deck. The objective is to score more than 60 points (out of 120 total).

### Card Values

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CARD HIERARCHY                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   CARICHI (High Value)          FIGURE (Face Cards)             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  A  â”‚ â”‚  3  â”‚               â”‚  K  â”‚ â”‚  H  â”‚ â”‚  J  â”‚        â”‚
â”‚   â”‚ 11p â”‚ â”‚ 10p â”‚               â”‚ 4p  â”‚ â”‚ 3p  â”‚ â”‚ 2p  â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â”‚   LISCI (Zero Points)                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚  7  â”‚ â”‚  6  â”‚ â”‚  5  â”‚ â”‚  4  â”‚ â”‚  2  â”‚                      â”‚
â”‚   â”‚ 0p  â”‚ â”‚ 0p  â”‚ â”‚ 0p  â”‚ â”‚ 0p  â”‚ â”‚ 0p  â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                 â”‚
â”‚   Rank Order: A > 3 > K > H > J > 7 > 6 > 5 > 4 > 2            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rules Summary

| Rule | Description |
|------|-------------|
| **Setup** | Each player gets 3 cards; one card revealed as trump (briscola) |
| **Play** | No obligation to follow suit - play any card |
| **Trump** | Any briscola beats any non-briscola card |
| **Same Suit** | Higher rank wins |
| **Different Suits** | First card wins (unless trump played) |
| **Drawing** | Winner draws first, then opponent |
| **Victory** | First to 61+ points wins; 60-60 is a tie |

### The Four Suits

| Italian | English | Symbol |
|---------|---------|--------|
| Denari | Coins | ğŸª™ |
| Coppe | Cups | ğŸ† |
| Spade | Swords | âš”ï¸ |
| Bastoni | Clubs | ğŸªµ |

---

## Technical Deep Dive

### Neural Network Architecture

The agent uses an **Actor-Critic** architecture with shared feature extraction:

```
                         INPUT
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Observation (533)   â”‚
              â”‚                        â”‚
              â”‚  â€¢ Hand: 120 dims      â”‚
              â”‚  â€¢ Trump: 4 dims       â”‚
              â”‚  â€¢ Played: 40 dims     â”‚
              â”‚  â€¢ Trick: 40 dims      â”‚
              â”‚  â€¢ Score: 1 dim        â”‚
              â”‚  â€¢ History: 328 dims   â”‚
              â”‚    (4 tricks Ã— 82)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Shared Feature       â”‚
              â”‚   Extractor            â”‚
              â”‚                        â”‚
              â”‚   Linear(533 â†’ 128)    â”‚
              â”‚   ReLU                 â”‚
              â”‚   Linear(128 â†’ 128)    â”‚
              â”‚   ReLU                 â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                           â”‚
            â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ACTOR HEAD       â”‚   â”‚      CRITIC HEAD      â”‚
â”‚                       â”‚   â”‚                       â”‚
â”‚   Linear(128 â†’ 64)    â”‚   â”‚   Linear(128 â†’ 64)    â”‚
â”‚   ReLU                â”‚   â”‚   ReLU                â”‚
â”‚   Linear(64 â†’ 3)      â”‚   â”‚   Linear(64 â†’ 1)      â”‚
â”‚                       â”‚   â”‚                       â”‚
â”‚   Output: Ï€(a|s)      â”‚   â”‚   Output: V(s)        â”‚
â”‚   (action probs)      â”‚   â”‚   (state value)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Trick History Encoding

A key design innovation is the **explicit trick history encoding**, which allows the agent to learn temporal patterns without requiring recurrent architectures (LSTM/GRU). 

#### Why History Matters

In Briscola, knowing *which* cards have been played is not enoughâ€”*how* they were played matters:

| Pattern | Strategic Implication |
|---------|----------------------|
| Opponent led with Ace | They may be "fishing" for your trump |
| They used trump early | Likely low on trump cards now |
| Won a trick with a low card | May have stronger cards in reserve |
| Lost a valuable card | Potential desperation play |

#### Encoding Structure

Each trick record (82 dimensions) encodes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRICK RECORD (82 dims)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Leader's Card          Responder's Card                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ One-hot: 40  â”‚       â”‚ One-hot: 40  â”‚                       â”‚
â”‚   â”‚ (card ID)    â”‚       â”‚ (card ID)    â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                 â”‚
â”‚   Who Led?               Who Won?                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ Binary: 1    â”‚       â”‚ Binary: 1    â”‚                       â”‚
â”‚   â”‚ (0=me, 1=opp)â”‚       â”‚ (0=me, 1=opp)â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                 â”‚
â”‚   Total: 40 + 40 + 1 + 1 = 82 dimensions per trick              â”‚
â”‚   History: 4 tricks Ã— 82 = 328 dimensions                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The agent maintains a rolling window of the last 4 tricks, enabling it to:
- Track opponent's card depletion patterns
- Recognize when opponent is conserving trump
- Learn counter-strategies based on recent play style
- Anticipate aggressive vs defensive opponents

### PPO Algorithm

The core learning algorithm uses the **clipped surrogate objective**:

```
L(Î¸) = E[ min(r(Î¸)Â·A, clip(r(Î¸), 1-Îµ, 1+Îµ)Â·A) ]

where:
  r(Î¸) = Ï€_new(a|s) / Ï€_old(a|s)    # probability ratio
  A    = advantage estimate          # how good was this action?
  Îµ    = 0.2                         # clipping parameter
```

**Why clipping matters**: Prevents the policy from changing too drastically in a single update, which could destabilize learning.

### Generalized Advantage Estimation (GAE)

GAE balances bias and variance in advantage estimation:

```
A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...

where:
  Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)   # TD error
  Î³   = 0.99                          # discount factor
  Î»   = 0.95                          # GAE parameter
```

| Î» Value | Trade-off |
|---------|-----------|
| 0.0 | High bias, low variance (TD learning) |
| 1.0 | Low bias, high variance (Monte Carlo) |
| 0.95 | Balanced (recommended) |

### Reward System

The reward system teaches strategic Briscola play through carefully designed signals:

#### Game Outcome Rewards

| Result | Reward | Description |
|--------|--------|-------------|
| Cappotto (120-0) | **Â±3.0** | Perfect game |
| Dominant (â‰¥91 pts) | **Â±2.5** | Strong victory |
| Solid (â‰¥80 pts) | **Â±2.0** | Clear victory |
| Standard win/loss | **Â±1.0 to Â±2.0** | Margin-scaled |
| Tie (60-60) | **0.0** | Draw |

#### Per-Trick Rewards

**Positive Signals:**
```
âœ“ Capturing opponent's high cards (Ace, Three)
âœ“ Winning tricks efficiently (low card beats high card)
âœ“ Leading with low-value cards (lisci)
âœ“ Strategic trump usage
```

**Penalties:**
```
âœ— Wasting carichi (A, 3) on worthless tricks
âœ— Unnecessary trump usage on low-value tricks
âœ— Missing capture opportunities with small trump
âœ— Leading with valuable cards (exposes them)
```

#### Card Strategic Weights

```python
WEIGHTS = {
    ACE:   1.00,  # Most valuable - protect and capture
    THREE: 0.90,  # Second most valuable
    KING:  0.35,  # Face card
    HORSE: 0.25,  # Face card
    JACK:  0.18,  # Face card
    SEVEN: 0.05,  # Best liscio
    SIX:   0.04,  # Liscio
    FIVE:  0.03,  # Liscio
    FOUR:  0.02,  # Liscio
    TWO:   0.01,  # Lowest value
}
```

---

## Web Application

### Running with Docker

```bash
# Start all services
docker compose up

# Access points:
# â€¢ Frontend: http://localhost
# â€¢ API Docs: http://localhost:8000/docs
```

### Running Locally

```bash
# Terminal 1: Start API
python -m api.run

# Terminal 2: Start frontend
cd frontend
npm install
npm start
```

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/game/new` | Start a new game |
| `GET` | `/game/{id}/state` | Get current game state |
| `POST` | `/game/{id}/play` | Play a card |
| `GET` | `/game/{id}/history` | Get game history |
| `DELETE` | `/game/{id}` | End game session |
| `POST` | `/model/load` | Load a checkpoint |
| `GET` | `/model/status` | Get model info |

---

## Project Structure

```
briscola-1-vs-1/
â”‚
â”œâ”€â”€ src/                          # Core game and RL logic
â”‚   â”œâ”€â”€ cards.py                  # Card, Deck classes
â”‚   â”œâ”€â”€ briscola_env.py           # RL environment
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ actor_critic.py       # Neural network
â”‚   â”‚   â”œâ”€â”€ ppo.py                # PPO algorithm
â”‚   â”‚   â”œâ”€â”€ gae.py                # Advantage estimation
â”‚   â”‚   â””â”€â”€ replay_buffer.py      # Experience storage
â”‚   â””â”€â”€ agents/
â”‚       â””â”€â”€ llm_opponent.py       # BAML/Ollama opponent
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train_vs_llm.py           # Training loop
â”‚
â”œâ”€â”€ briscola_rl/
â”‚   â””â”€â”€ cli.py                    # Command-line interface
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                   # FastAPI application
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/                     # Angular application
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ components/           # UI components
â”‚   â”‚   â”œâ”€â”€ services/             # API services
â”‚   â”‚   â””â”€â”€ models/               # TypeScript interfaces
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ baml_src/                     # LLM function definitions
â”‚   â”œâ”€â”€ clients.baml              # Ollama client config
â”‚   â””â”€â”€ briscola_functions.baml   # Card selection logic
â”‚
â”œâ”€â”€ checkpoints/                  # Saved model weights
â”œâ”€â”€ Modelfile                     # Ollama model definition
â”œâ”€â”€ docker-compose.yml            # Container orchestration
â”œâ”€â”€ pyproject.toml                # Python package config
â””â”€â”€ requirements.txt              # Dependencies
```

---

## Configuration

### Training Hyperparameters

```bash
briscola train \
  --num-updates 1000 \        # Total training iterations
  --episodes-per-update 10 \  # Games per PPO update
  --lr 3e-4 \                 # Learning rate
  --gamma 0.99 \              # Discount factor
  --clip-epsilon 0.2 \        # PPO clipping
  --hidden-dim 128 \          # Network size
  --device cpu                # cpu or cuda
```

### Recommended Settings by Goal

| Goal | Updates | Episodes/Update | Hidden Dim |
|------|---------|-----------------|------------|
| Quick test | 100 | 5 | 64 |
| Standard training | 1000 | 10 | 128 |
| Best performance | 5000 | 20 | 256 |

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434/v1` | Ollama server URL |
| `ENVIRONMENT` | `development` | API environment |

---

## Contributing

### Development Setup

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run linter
ruff check .

# Run tests
pytest

# Auto-fix issues
ruff check --fix .
```

### Code Style

This project uses [Ruff](https://github.com/astral-sh/ruff) for linting with a 100-character line limit.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.